# Qwen3.5-27B M2 24GB 評価レポート

**日付**: 2026-03-01
**環境**: Apple M2 MacBook Air 24GB / macOS 15.5 / Ollama 0.17.4

## 目的

umigame（水平思考パズル）サービス向けに、Qwen3.5-27B を M2 24GB 環境で実用的に動作させることが可能か検証する。ベースラインは gemma3:4b（28-30 tok/s, swap 0）。

---

## 1. ベンチマーク結果

### 1.1 公式モデル比較（Ollama Q4_K_M）

| モデル | サイズ | tok/s | swap | レイテンシ |
|--------|--------|-------|------|-----------|
| gemma3:4b | 3.3GB | 28-32 | 0 | 1-2s |
| gemma3:12b | 8.1GB | 10-12 | 0 | 3-5s |
| qwen3.5:27b | 17GB | 2.5-3.0 | 3GB | 30-40s |

### 1.2 日本語品質比較（umigame ゲームマスター）

テストシナリオ: 海辺でウミガメのスープを注文した男が泣いた理由

| モデル | 正答率 | 隠し情報漏洩 | 日本語品質 | JSON遵守 |
|--------|--------|-------------|-----------|---------|
| gemma3:4b | 2/3 | **漏洩あり** | 良好 | 良好 |
| gemma3:12b | 1/3 | なし | 不自然 | 良好 |
| qwen3.5:27b | **3/3** | **なし** | **優秀** | 良好 |

**所見**: Qwen3.5-27B は品質面で最も優秀だが、2.5-3.0 tok/s + 3GB swap では実用不可能。

### 1.3 Qwen3.5 thinking モード問題

`think=false` を Ollama API で指定しても、`<think>` ブロックが出力に含まれるケースが確認された。全トークンが思考プロセスに消費され、実質的な応答が空になる問題。公式モデルパラメータ使用で部分的に緩和可能。

---

## 2. 均衡プルーニング実験

### 2.1 手法

arXiv:2512.22106 に基づくゲーム理論的均衡プルーニング。FP16 safetensors をシャードごとに処理。

```
アルゴリズム: 参加変数 s ∈ [0,1] を各重みに割当て、
  benefit = α * (|w| * w)
  cost = 2β * w² * s
  L1 = γ * 1(s > 0)
  s ← clamp(s + lr * (benefit - cost - L1), 0, 1)
→ s が閾値以下の重みをゼロ化
```

**レイヤー別スパース率**:

| ターゲット | スパース率 |
|-----------|-----------|
| attn q/k/v/o_proj | 25% |
| mlp gate/up/down_proj | 30% |
| embed_tokens / lm_head / norm | 0%（除外）|

### 2.2 結果

- **達成スパース率**: 20.37%（非構造的）
- **処理**: MPS GPU 使用、11 シャード × 各 5.3GB
- **問題**: 非構造的スパース化ではゼロ化された重みも GGUF ブロック量子化でエンコードされるため、**ファイルサイズは減少しない**

### 2.3 GGUF 変換結果

| 変換 | サイズ | 備考 |
|------|--------|------|
| プルーニング前 Q4_K_M | 17GB | 公式 Ollama モデル |
| プルーニング後 Q4_K_M | 15.8GB | テンソル数不足で Ollama ロード不可 |
| プルーニング後 IQ4_XS | 14.2GB | 同上 |

**失敗原因**: Homebrew 版 `convert_hf_to_gguf.py` が Qwen3.5 のハイブリッドアーキテクチャ（Mamba SSM + Transformer + MTP）に完全対応しておらず、851/1307 テンソルしか変換されなかった。

---

## 3. 1.58-bit 量子化調査

### 3.1 調査経緯

| 手法 | ツール | 結果 |
|------|--------|------|
| TERNARY (BitNet b1.58) | llm-quantize ultra_low_bit.py | FP32 全量ロード必要 (108GB) → 不可能 |
| TQ1_0 (1.69 bpw) | llama-quantize | GGUF 生成成功 (6.9GB) → ランタイム非対応 |
| IQ1_S (1.56 bpw) | llama-quantize | imatrix 必要 → /tmp 消失で中断 |
| Q2_K (3.08 bpw) | llama-quantize | 動作するが品質壊滅的 |

### 3.2 TQ1_0 詳細

**GGUF 生成**:
```
入力: Ollama qwen3.5:27b Q4_K_M (17GB, 1307 テンソル)
出力: TQ1_0 GGUF (6.9GB, 2.14 BPW)
```

**前処理**: Ollama GGUF の `rope.dimension_sections` が `[11,11,10]`（3要素）だが、Homebrew llama.cpp は 4 要素を要求。バイナリパッチで `[11,11,10,0]` に修正（アラインメント調整込み）。

**ランタイムブロッカー**:

1. **Homebrew llama-server**: `missing tensor 'blk.0.ssm_dt.bias'`
   - Ollama GGUF: `blk.0.ssm_dt`（バイアスなし）
   - Homebrew: `blk.0.ssm_dt.bias`（バイアス付き）を期待
   - 原因: Ollama と Homebrew llama.cpp の SSM テンソル命名規則の分岐

2. **Ollama ランナー**: TQ1_0 テンソルタイプで segfault
   - `ollama create` は成功するが、推論時にクラッシュ（exit status 2）
   - 原因: Ollama の qwen35 ランナーが TQ1_0 型をサポートしていない

### 3.3 Q2_K 品質評価

動作はするが品質壊滅:
- 日本語: 無限文字繰返しループ（「男」「は」「男」「は」...）
- 推論: 矛盾する循環的テキスト生成
- 判定: **使用不可** — 3.08 bpw は Qwen3.5-27B には不十分

### 3.4 理論的な 1.58-bit 配置

TQ1_0 (6.9GB) が動作した場合のメモリ見積:

| 項目 | サイズ |
|------|--------|
| モデル重み | 6.9GB |
| KV キャッシュ (512 ctx) | ~0.5GB |
| ワーキングメモリ | ~1GB |
| **合計** | **~8.4GB** |
| GPU メモリ (M2) | 16GB |
| **余裕** | **~7.6GB** |

→ swap なしで完全にメモリに収まり、10+ tok/s が理論的に達成可能。

### 3.5 TQ1_0 の根本的制約

llama.cpp GitHub issue #15193 により判明: **TQ1_0 は三値重み {-1,0,+1} で訓練されたモデル専用**。通常の FP16/FP32 モデルを事後的に TQ1_0 に量子化しても、品質は壊滅する。Q2_K (3.08 bpw) で品質壊滅だったことと整合する — 事後量子化ではこのビット幅が限界。

1.58-bit で品質を維持するには、**モデル自体が三値重みで訓練される必要がある**（BitNet b1.58 アプローチ）。

---

## 4. Microsoft BitNet (bitnet.cpp) 調査

### 4.1 概要

[microsoft/BitNet](https://github.com/microsoft/BitNet) は Microsoft Research が開発した 1.58-bit LLM 専用推論エンジン。llama.cpp の TQ1_0 とは根本的にアプローチが異なる。

| 項目 | bitnet.cpp | llama.cpp TQ1_0 |
|------|-----------|----------------|
| 対象モデル | 三値訓練済みモデル専用 | 任意モデルの事後量子化 |
| 推論カーネル | **LUT ベース**（ルックアップテーブル） | MAD ベース（乗算-加算） |
| M2 速度 (3B) | ~45 tok/s | ~30 tok/s |
| M2 速度 (100B 理論値) | 5-7 tok/s | N/A |
| エネルギー効率 | 55-82% 削減 | 標準 |
| Ollama 統合 | なし | なし (qwen35 非対応) |

### 4.2 LUT カーネルの優位性

bitnet.cpp は 3 種類のカーネルを提供:

| カーネル | 方式 | 特徴 |
|---------|------|------|
| I2_S | 2-bit MAD | 高スレッド環境向け |
| TL1 | 4-bit LUT (9 要素テーブル) | ARM/メモリ制約向け |
| TL2 | 5-bit LUT (3 重み/5bit) | 最高圧縮率、M-series 最適 |

**LUT が MAD より高速な理由**: 三値重み {-1,0,+1} では乗算が不要（加算/減算/ゼロのみ）。LUT は入力活性値の事前計算テーブルを使い、乗算を完全に排除する。Intel i7 で llama.cpp TQ1_0 比 2.32x、M2 Ultra で 1.19x の高速化。

### 4.3 利用可能なモデル

| モデル | パラメータ | ソース |
|--------|-----------|--------|
| bitnet-b1.58-2B-4T | 2B | Microsoft 公式 (4T トークン訓練) |
| bitnet_b1_58-3B | 3.3B | コミュニティ再現 |
| bitnet_b1_58-large | 0.7B | コミュニティ再現 |

**27B クラスの三値訓練モデルは 2026年3月時点で存在しない**。

### 4.4 umigame への適用可能性

| 条件 | 状況 |
|------|------|
| 日本語品質 | 未検証（2B モデルでは期待薄） |
| モデルサイズ | 最大 3.3B — umigame には不十分な可能性 |
| Ollama 統合 | **非対応** — 独自サーバー実装が必要 |
| Qwen 互換 | **なし** — BitNet 専用アーキテクチャ |

**結論**: bitnet.cpp は技術的に優れているが、現時点では umigame 要件（日本語品質、27B クラス推論能力）を満たすモデルが存在しない。27B クラスの三値訓練モデルが公開された場合、bitnet.cpp は M2 24GB で 5-7 tok/s（swap なし）が理論的に実現可能。

---

## 5. 事後量子化 vs 三値訓練: 根本的な分岐

本評価で最も重要な知見は、1.58-bit 推論には**2つの根本的に異なるアプローチ**が存在するということ:

| | 事後量子化 (PTQ) | 三値訓練 (QAT) |
|---|---|---|
| 方式 | FP16 モデル → TQ1_0/IQ1_S に変換 | 訓練時から {-1,0,+1} で重み更新 |
| 品質 | **壊滅的劣化** (27B で確認済み) | 品質維持 (BitNet b1.58 論文) |
| ツール | llama-quantize, llm-quantize | BitNet 訓練フレームワーク |
| 推論 | llama.cpp (MAD カーネル) | bitnet.cpp (LUT カーネル) |
| 現実性 | 道具はあるがモデル品質が不足 | モデルが小さい (最大 3B) |

**Qwen3.5-27B を 1.58-bit で実用化するには、Qwen3.5 自体が三値訓練される必要がある。** これは Alibaba (Qwen チーム) または Microsoft (BitNet チーム) の判断に依存し、llm-quantize 側で解決できる問題ではない。

---

## 6. アーキテクチャ上の障壁

### 6.1 Qwen3.5 ハイブリッドアーキテクチャ

Qwen3.5-27B は標準的な Transformer ではなく、**Gated DeltaNet** ベースのハイブリッドアーキテクチャ:

- **64 レイヤー** (3 × linear_attention + 1 × full_attention) × 16
- **Linear Attention (48層)**: Gated DeltaNet — in_proj_qkv, in_proj_z, in_proj_a/b, A_log, dt_bias, conv1d
- **Full Attention (16層)**: 標準 GQA — Q=16 heads, KV=4 heads, head_dim=256
- **MTP (Multi-Token Prediction)**: 投機的デコーディング用ヘッド
- `full_attention_interval=4`: GGUF メタデータで固定パターンを表現

### 6.2 ツールチェーン分岐問題

| 項目 | Ollama | Homebrew llama.cpp |
|------|--------|-------------------|
| SSM テンソル名 | `ssm_dt` | `ssm_dt.bias` |
| RoPE dim sections | 3 要素 | 4 要素 |
| TQ1_0 対応 (qwen35) | 未対応 (segfault) | テンソル名不一致 |
| convert_hf_to_gguf | 完全対応 (1307 テンソル) | 部分対応 (851 テンソル) |

Ollama と Homebrew llama.cpp は独立にフォークされており、SSM テンソルの命名規則が異なる。標準 Transformer モデルでは問題ないが、Qwen3.5 のハイブリッドアーキテクチャでこの分岐が顕在化する。

---

## 7. 結論と推奨

### 7.1 現状の結論

| 判定 | 内容 |
|------|------|
| **品質** | Qwen3.5-27B は umigame で最高品質（3/3正答、漏洩なし） |
| **速度** | Q4_K_M (17GB) で 2.5-3.0 tok/s — 実用不可能 |
| **プルーニング** | 非構造的スパースは GGUF サイズに寄与しない |
| **1.58-bit** | TQ1_0 生成成功だがランタイム非対応 |
| **Q2_K** | 品質壊滅 — 使用不可 |
| **採用** | **現時点では不可**。gemma3:4b を継続使用 |

### 7.2 構造的プルーニング結果 → 品質壊滅

評価の結果を踏まえ、**構造的プルーニング + Q4_K_M** を実施した。パイプラインは技術的に成功したが、品質が壊滅した。

---

## 8. 構造的プルーニング実験

### 8.1 実装概要

`llm-pruning/scripts/structural_prune.py` で均衡プルーニングを構造的（ヘッド/チャネル単位）に拡張。

**パイプライン**:
```
FP16 safetensors (52GB)
  → 均衡ゲーム理論スコアリング (Pass 1)
  → グローバルランキング → プルーニングプラン生成
  → テンソル次元縮小 (Pass 2, in-place)
  → config.json 更新
  → GGUF F16 変換 (convert_hf_to_gguf.py)
  → Q4_K_M 量子化
```

### 8.2 プルーニング設定

| 対象 | Before | After | 削減 |
|------|--------|-------|------|
| MLP channels | 17408 | 12160 (30%) | 全64層 |
| Full Attn Q heads | 24 | 16 (33%) | 16層 |
| Linear V heads | 48 | 32 (33%) | 48層 |
| Layer removal | 64 | 64 (**0%**) | 自動無効化 |

**レイヤー除去が自動無効化された理由**: `full_attention_interval=4` のアーキテクチャでは、レイヤー削除が 3:1 の linear:full パターンを破壊し、GGUF メタデータと不整合になるため。

### 8.3 修正された4つのバグ

実行前に4つの重大バグを修正:

| # | バグ | 影響 | 修正 |
|---|------|------|------|
| 1 | DeltaNet テンソル未処理 | in_proj_z, in_proj_a/b, A_log, dt_bias, conv1d が未プルーニング | V-head 依存テンソルのハンドラ追加 |
| 2 | V-head 数が K-head で割り切れない | GGUF converter の reshape 失敗 | `n_keep = (n_keep // n_k) * n_k` で丸め |
| 3 | safetensors.index.json 未再生成 | HF loader がシャードを見つけられない | `regenerate_safetensors_index()` 追加 |
| 4 | レイヤー除去で interval パターン破壊 | GGUF の `full_attention_interval` と不整合 | `interval > 1` 時に自動無効化 |

テスト: 57 テスト全通過 (43 → 57、14テスト追加)

### 8.4 サイズ・速度結果

| 指標 | 元モデル (Q4_K_M) | プルーニング後 (Q4_K_M) | 目標 |
|------|-------------------|----------------------|------|
| パラメータ | 27B | ~19B | 17.5B |
| GGUF サイズ | 17GB | **12GB** | ≤ 10GB |
| BPW | 4.5 | 5.25 (q8_0 fallback) | 4.5 |
| tok/s | 2.5-3.0 | **5-10** | 8-12 |
| swap | 3GB | **0** | 0 |

**q8_0 fallback**: 12160 (pruned intermediate_size) が 256 で割り切れないため、64/498 テンソルが Q4_K_M ではなく q8_0 にフォールバック。結果として 5.25 BPW になり、目標の 10GB を超過。

### 8.5 品質結果 — 壊滅

| テスト | 結果 |
|--------|------|
| chat completion | `<think>` トークン2つで停止、応答なし |
| raw completion | `"S F.\nSpartan S.A.S.A.D.U.A.S.U.S."` — 無意味 |
| 日本語生成 | 不可能 |
| umigame 正答率 | **0/3** (応答生成自体が不可能) |

**結論**: 30% 構造的プルーニング（fine-tuning なし）は Qwen3.5-27B の品質を完全に破壊する。

### 8.6 失敗分析

構造的プルーニングが壊滅した原因:

1. **Fine-tuning なし**: 構造変更後に重みの再調整がなく、残存パラメータ間の整合性が崩壊
2. **DeltaNet 感度**: Gated DeltaNet の線形注意レイヤーは、V-head 削減に対して標準 Transformer より敏感な可能性
3. **削減率 30% が過大**: small model (~1B) で成功する削減率が 27B モデルで同じく成功するとは限らない
4. **均一削減の限界**: 全レイヤーで同じインデックスを削除する均一方式は、レイヤー間の役割差異を無視

### 8.7 知見まとめ

| 手法 | サイズ効果 | 速度効果 | 品質 | 判定 |
|------|-----------|---------|------|------|
| 非構造的プルーニング (20%) | なし | なし | 維持 | サイズ不変 |
| 構造的プルーニング (30%) | 17→12GB | 3→5-10 tok/s | **壊滅** | Fine-tuning 必須 |
| 事後量子化 Q2_K (3bpw) | 17→10GB | 3→5 tok/s | 壊滅 | ビット不足 |
| 事後量子化 TQ1_0 (1.7bpw) | 17→7GB | N/A (非対応) | N/A | ランタイム非対応 |
| **公式 Q4_K_M (4.5bpw)** | **17GB** | **2.5-3 tok/s** | **最高** | **品質唯一解** |

---

## 9. 最終結論

### 9.1 確定した事実

M2 24GB で Qwen3.5-27B を実用速度 (8+ tok/s) かつ実用品質で動作させる手段は、**2026年3月時点で存在しない**。

```
試行済み全手法:
  ✗ 事後量子化 3bpw 以下 → 品質壊滅 (Q2_K, TQ1_0)
  ✗ 非構造的プルーニング → GGUF サイズ不変
  ✗ 構造的プルーニング (fine-tuning なし) → 品質壊滅
  ✗ 三値訓練 (BitNet) → 27B モデル未公開

  ○ 公式 Q4_K_M → 品質最高だが 2.5-3 tok/s + 3GB swap
```

### 9.2 推奨

| 判定 | 内容 |
|------|------|
| **短期** | gemma3:4b を継続使用（28-30 tok/s, 漏洩対策強化） |
| **中期 (受動的)** | Qwen3.5-7B/14B 公開 or 三値訓練 27B 公開を待つ |
| **中期 (能動的)** | 構造的プルーニング + LoRA fine-tuning (要 GPU 環境) |
| **長期** | M4 Pro/Max (48GB+) で Q4_K_M をそのまま使用 |

### 9.3 代替案

| 代替 | 実現性 | 利点 | 欠点 |
|------|--------|------|------|
| gemma3:4b 継続 | 即時 | 28-30 tok/s, swap 0 | 隠し情報漏洩リスク |
| 構造的プルーニング + fine-tuning | 能動的 | ~10GB, 8-12 tok/s | GPU 環境必要 (A100 推奨) |
| IQ2_XXS + imatrix | 能動的 | ~7GB | 品質未検証 |
| bitnet.cpp + 三値訓練 27B | 受動的 | 5-7 tok/s, swap 0 | 27B モデル未公開 |
| Qwen3.5-7B/14B（公開待ち） | 受動的 | メモリに収まる可能性大 | 未公開 |
| クラウド推論 (API) | 即時 | 品質・速度問題なし | レイテンシ、コスト、プライバシー |

---

## 付録: 使用ツール・スクリプト

| ツール | 用途 |
|--------|------|
| `llm-pruning/scripts/prune_safetensors.py` | FP16 均衡プルーニング（非構造的） |
| `llm-pruning/scripts/structural_prune.py` | FP16 構造的プルーニング（ヘッド/チャネル/レイヤー） |
| `llama-quantize` (Homebrew / source build) | GGUF 量子化 (Q4_K_M, TQ1_0, Q2_K) |
| `llama-server` (source build) | 推論サーバー（Ollama 非対応時のフォールバック） |
| `convert_hf_to_gguf.py` (llama.cpp) | HF safetensors → GGUF 変換 |
| GGUF バイナリパッチャー (インライン) | rope.dimension_sections 修正 |
| Ollama 0.17.4 | モデル管理・推論（公式モデルのみ安定）|
